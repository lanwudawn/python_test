import streamlit as st
import cv2
import tempfile
import os
from pathlib import Path
import time
import numpy as np
from PIL import Image
import pandas as pd
from datetime import datetime

from ..models.yolo_model import YOLOModel
from ..utils.config_loader import load_config
from ..reports.visualization import DetectionVisualizer
from ..reports.metrics import DetectionMetrics

class StreamlitApp:
    def __init__(self):
        self.setup_page_config()
        self.config = load_config()
        self.apply_custom_css()
        self.initialize_session_state()

    def setup_page_config(self):
        """è®¾ç½®é¡µé¢é…ç½®"""
        st.set_page_config(
            page_title="AIçš„ç›®æ ‡æ£€æµ‹ç³»ç»Ÿ",
            page_icon="ğŸ¯",
            layout="wide",
            initial_sidebar_state="expanded"
        )

    def apply_custom_css(self):
        """åº”ç”¨è‡ªå®šä¹‰CSSæ ·å¼"""
        st.markdown("""
        <style>
        /* ä¸»é¢˜é¢œè‰² */
        :root {
            --primary-bg: #FEFCE8;
            --secondary-bg: #FFFFFF;
            --accent-color: #F59E0B;
            --text-color: #1F2937;
            --card-bg: #FEF3C7;
        }
        
        /* å…¨å±€æ ·å¼ */
        .stApp {
            background: linear-gradient(135deg, var(--primary-bg) 0%, var(--secondary-bg) 100%);
            color: var(--text-color);
        }
        
        /* ä¾§è¾¹æ æ ·å¼ */
        .css-1d391kg {
            background-color: var(--card-bg);
        }
        
        /* å¡ç‰‡æ ·å¼ */
        .card {
            background: var(--card-bg);
            border-radius: 10px;
            padding: 1.5rem;
            border: 1px solid rgba(255, 255, 255, 0.1);
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            margin-bottom: 1rem;
            backdrop-filter: blur(10px);
        }
        
        /* æ ‡é¢˜æ ·å¼ */
        h1 {
            background: linear-gradient(90deg, #1E88E5, #64B5F6);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            font-weight: 700;
            font-size: 2.5rem;
            margin-bottom: 2rem;
        }
        
        h2, h3 {
            color: var(--text-color);
            font-weight: 500;
        }
        
        /* æŒ‰é’®æ ·å¼ */
        .stButton>button {
            background: linear-gradient(90deg, #1E88E5, #64B5F6);
            color: white;
            border: none;
            padding: 0.5rem 2rem;
            border-radius: 5px;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        
        .stButton>button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(30, 136, 229, 0.3);
        }
        
        /* æŒ‡æ ‡å¡ç‰‡æ ·å¼ */
        .metric-card {
            background: var(--card-bg);
            border-radius: 10px;
            padding: 1.5rem;
            text-align: center;
            border: 1px solid rgba(255, 255, 255, 0.1);
            transition: transform 0.3s ease;
        }
        
        .metric-card:hover {
            transform: translateY(-5px);
        }
        
        .metric-value {
            font-size: 2.5rem;
            font-weight: 700;
            color: #1E88E5;
            margin: 0.5rem 0;
        }
        
        .metric-label {
            color: var(--text-color);
            font-size: 1rem;
            opacity: 0.8;
        }
        
        /* è¿›åº¦æ¡æ ·å¼ */
        .stProgress > div > div > div {
            background: linear-gradient(90deg, #1E88E5, #64B5F6);
        }
        
        /* é€‰æ‹©å™¨æ ·å¼ */
        .stSelectbox > div > div {
            background-color: var(--card-bg);
            border: 1px solid rgba(255, 255, 255, 0.1);
        }
        
        /* å›¾è¡¨æ ·å¼ */
        .stPlot {
            background: var(--card-bg);
            border-radius: 10px;
            padding: 1rem;
        }
        
        /* åŠ¨ç”»æ•ˆæœ */
        @keyframes pulse {
            0% { box-shadow: 0 0 0 0 rgba(30, 136, 229, 0.4); }
            70% { box-shadow: 0 0 0 10px rgba(30, 136, 229, 0); }
            100% { box-shadow: 0 0 0 0 rgba(30, 136, 229, 0); }
        }
        
        .detection-active {
            animation: pulse 2s infinite;
        }
        
        /* ä¸Šä¼ åŒºåŸŸæ ·å¼ */
        .uploadfile {
            border: 2px dashed rgba(255, 255, 255, 0.2);
            border-radius: 10px;
            padding: 2rem;
            text-align: center;
            transition: all 0.3s ease;
        }
        
        .uploadfile:hover {
            border-color: var(--accent-color);
        }
        
        /* è¡¨æ ¼æ ·å¼ */
        .dataframe {
            background: var(--card-bg);
            border-radius: 10px;
            border: 1px solid rgba(255, 255, 255, 0.1);
        }
        </style>
        """, unsafe_allow_html=True)

    def initialize_session_state(self):
        """åˆå§‹åŒ–ä¼šè¯çŠ¶æ€"""
        if 'model' not in st.session_state:
            st.session_state.model = None
        if 'metrics' not in st.session_state:
            st.session_state.metrics = DetectionMetrics()
        if 'running' not in st.session_state:
            st.session_state.running = False

    def run(self):
        """è¿è¡ŒStreamlitåº”ç”¨"""
        self.render_header()
        
        # åˆ›å»ºæ ‡ç­¾é¡µ
        tabs = st.tabs([
            "ğŸ¥ è§†é¢‘ç›®æ ‡æ£€æµ‹",
            "ğŸ–¼ï¸ å›¾ç‰‡åˆ†æ",
            "ğŸ“Š åˆ†æå¤§å±"
        ])
        
        with tabs[0]:
            self.render_realtime_detection()
        with tabs[1]:
            self.render_image_detection()
        with tabs[2]:
            self.render_analytics()

    def render_header(self):
        """æ¸²æŸ“é¡µé¢å¤´éƒ¨"""
        st.markdown("""
        <div style="text-align: center; padding: 2rem 0;">
            <h1>AIçš„ç›®æ ‡æ£€æµ‹</h1>
            <p style="color: #64B5F6; font-size: 1.2rem;">
                ç›®æ ‡æ£€æµ‹åŠåˆ†æ
            </p>
        </div>
        """, unsafe_allow_html=True)

    def render_metric_card(self, title, value, icon):
        """æ¸²æŸ“æŒ‡æ ‡å¡ç‰‡"""
        st.markdown(f"""
        <div class="metric-card">
            <div style="font-size: 2rem; color: #64B5F6; margin-bottom: 0.5rem;">
                {icon}
            </div>
            <div class="metric-value">{value}</div>
            <div class="metric-label">{title}</div>
        </div>
        """, unsafe_allow_html=True)

    def render_realtime_detection(self):
        """æ¸²æŸ“å®æ—¶æ£€æµ‹é¡µé¢"""
        col1, col2 = st.columns([6, 4])
        
        with col1:
            st.markdown('<div class="card">', unsafe_allow_html=True)
            video_placeholder = st.empty()
            video_placeholder.write("å¯¼å…¥æ–‡ä»¶")
            # æ·»åŠ çŠ¶æ€æŒ‡ç¤ºå™¨
            if st.session_state.running:
                st.markdown("""
                    <div style="text-align: center; color: #1E88E5;">
                        ğŸ”´ æ­£åœ¨å½•åˆ¶
                    </div>
                """, unsafe_allow_html=True)
            st.markdown('</div>', unsafe_allow_html=True)

        with col2:
            st.markdown('<div class="card">', unsafe_allow_html=True)
            st.markdown("### æ§åˆ¶ç•Œé¢")
            source = st.radio(
                "Select Input Source",
                ["ğŸ“¹ æ‘„åƒå¤´", "ğŸ“ è§†é¢‘æ–‡ä»¶"],
                key="source_select"
            )

            if source == "ğŸ“¹ æ‘„åƒå¤´":
                camera_id = st.selectbox("é€‰æ‹©é€šé“", [0, 1, 2])
                self.run_camera_detection(camera_id, video_placeholder)
            else:
                st.markdown('<div class="uploadfile">', unsafe_allow_html=True)
                video_file = st.file_uploader(
                    "æ‹–æ‹½æ–‡ä»¶åˆ°æ­¤",
                    type=['mp4', 'avi', 'mov']
                )
                st.markdown('</div>', unsafe_allow_html=True)

                if video_file:
                    self.run_video_detection(video_file, video_placeholder)

            st.markdown('</div>', unsafe_allow_html=True)

    def render_image_detection(self):
        """æ¸²æŸ“å›¾ç‰‡æ£€æµ‹é¡µé¢"""
        st.markdown('<div class="card">', unsafe_allow_html=True)
        st.markdown("### å›¾ç‰‡åˆ†æ")
        
        upload_col, preview_col = st.columns([3,7])
        
        with upload_col:
            st.markdown('<div class="uploadfile">', unsafe_allow_html=True)
            uploaded_file = st.file_uploader(
                "æ‹–æ‹½æ–‡ä»¶åˆ°æ­¤",
                type=['jpg', 'jpeg', 'png']
            )
            st.markdown('</div>', unsafe_allow_html=True)
            
        if uploaded_file:
            image = Image.open(uploaded_file)
            preview_col.image(image, caption="Preview", use_container_width=True)
            
            if st.button("ğŸ” åˆ†ææ–‡ä»¶"):
                with st.spinner("åˆ†æä¸­..."):
                    result_image = self.process_image(uploaded_file)
                    st.image(result_image, caption="Detection Result")
        
        st.markdown('</div>', unsafe_allow_html=True)

    def render_analytics(self):
        """æ¸²æŸ“åˆ†æé¡µé¢"""
        if st.session_state.metrics:
            metrics = st.session_state.metrics.get_summary()
            
            # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
            col1, col2, col3 = st.columns(3)
            with col1:
                self.render_metric_card(
                    "æ€»è®¡ç”»å¹…",
                    metrics['total_frames'],
                    "ğŸï¸"
                )
            with col2:
                self.render_metric_card(
                    "å¹³å‡å¸§ç‡",
                    f"{metrics['average_fps']:.1f}",
                    "âš¡"
                )
            with col3:
                self.render_metric_card(
                    "æ£€æµ‹ç›®æ ‡",
                    metrics['total_detections'],
                    "ğŸ¯"
                )
            
            # æ˜¾ç¤ºå›¾è¡¨
            st.markdown('<div class="card">', unsafe_allow_html=True)
            st.markdown("### ç›®æ ‡åˆ†ç±»")
            if metrics['class_distribution']:
                chart_data = pd.DataFrame.from_dict(
                    metrics['class_distribution'],
                    orient='index',
                    columns=['count']
                )
                st.bar_chart(chart_data)
            st.markdown('</div>', unsafe_allow_html=True)

    def process_image(self, image_file):
        """å¤„ç†å•å¼ å›¾ç‰‡"""
        if st.session_state.model is None:
            st.session_state.model = YOLOModel(self.config['model'])
        
        image = Image.open(image_file)
        image = np.array(image)
        
        detections = st.session_state.model.predict(image)
        visualizer = DetectionVisualizer()
        
        return visualizer.draw_detections(image, detections)

    def run_camera_detection(self, camera_id, placeholder):
        """è¿è¡Œæ‘„åƒå¤´æ£€æµ‹"""
        cap = cv2.VideoCapture(camera_id)
        visualizer = DetectionVisualizer()
        
        if st.session_state.model is None:
            st.session_state.model = YOLOModel(self.config['model'])
        
        # åˆ›å»ºä¸¤åˆ—å¸ƒå±€ç”¨äºå¼€å§‹å’Œåœæ­¢æŒ‰é’®
        col1, col2 = st.columns(2)
        
        with col1:
            start_button = st.button("â–¶ï¸ å¼€å§‹")
        with col2:
            stop_button = st.button("â¹ï¸ ç»“æŸ")
            
        if start_button:
            st.session_state.running = True
            
        if stop_button:
            st.session_state.running = False
            
        try:
            while st.session_state.running:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                detections = st.session_state.model.predict(frame)
                
                if detections is not None:
                    frame = visualizer.draw_detections(frame, detections)
                
                placeholder.image(frame)
                
        finally:
            cap.release()
            st.session_state.running = False

    def run_video_detection(self, video_file, placeholder):
        """è¿è¡Œè§†é¢‘æ–‡ä»¶æ£€æµ‹"""
        tfile = tempfile.NamedTemporaryFile(delete=False)
        tfile.write(video_file.read())
        
        cap = cv2.VideoCapture(tfile.name)
        visualizer = DetectionVisualizer()
        
        if st.session_state.model is None:
            st.session_state.model = YOLOModel(self.config['model'])
        
        # åˆ›å»ºä¸¤åˆ—å¸ƒå±€ç”¨äºå¼€å§‹å’Œåœæ­¢æŒ‰é’®
        col1, col2 = st.columns(2)
        
        with col1:
            start_button = st.button("â–¶ï¸ å¼€å§‹")
        with col2:
            stop_button = st.button("â¹ï¸ ç»“æŸ")
            
        if start_button:
            st.session_state.running = True
            
        if stop_button:
            st.session_state.running = False
            
        try:
            while st.session_state.running:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                detections = st.session_state.model.predict(frame)
                
                if detections is not None:
                    frame = visualizer.draw_detections(frame, detections)
                
                placeholder.image(frame)
                
        finally:
            cap.release()
            os.unlink(tfile.name)
            st.session_state.running = False 